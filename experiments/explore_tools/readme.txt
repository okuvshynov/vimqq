Rather than providing context manually, let's ask model what it needs to answer it. 
We can provide.

For example, if we want to get plaintext index for a codebase for vscode, we'll need to process 30M+ tokens. That's not too bad I guess?


