*vimqq.txt*  For Vim version 8.0  Last change: 2024 July 23

VIMQQ ~

Author: Oleksandr Kuvshynov

AI plugin with a focus on local model evaluation, code reading, refinement and
coaching rather than code completion/implementing something e2e.

1. Introduction ........................................ |vimqq-intro|
2. Installation ........................................ |vimqq-install|
3. Usage ............................................... |vimqq-usage|
4. Commands ............................................ |vimqq-commands|
  4.1. Message sending ................................. |vimqq-commands-msg|
  4.2. Forking ......................................... |vimqq-commands-fork|
  4.3. UI Controls ..................................... |vimqq-commands-ui|
5. Mappings ............................................ |vimqq-mappings|
6. Configuration ....................................... |vimqq-config|
7. Changelog ........................................... |vimqq-changelog|

==============================================================================
1. Introduction                                                    *vimqq-intro*

LLMs appear to be very useful in different cases:
 - explaining what some existing code does; 
 - suggesting some existing tools/abstractions to use to improve the code;
 - being a first line of code review - finding glaring errors;
 - being a tutor or coach - here's some code I've written, what should I
   learn to improve?
 - early prototyping;

What vimqq is not doing:
 - generating code in place, typing it in editor directly, all communication
   is done in the chat buffer. It is reasonably easy to copy/paste the code.
 - providing any form of autocomplete.  

Features:
 - Support for both Claude remote models through paid API and local models 
   via llama.cpp server (or compatible);
 - streaming response from llama.cpp server, so that user can start
   reading it as it is being generated. Llama3-70B can produce 8-10 tps 
   on Apple M2 Ultra, which is very close to human reading rate. 
   This way user will not waste any time waiting for reply;
 - flexible extra context - visual selection, ctags, current file, entire
   project and combination of the above.
 - KV cache warmup for llama.cpp. In cases of high-memory but low-compute
   hardware configuration for LLM inference (Apple devices, CPU-only machines)
   processing original prompt might take a while in cases of large context 
   or long chat history. To help with that and further amortize the cost,
   it is possible to send and automate sending warmup queries to prefill
   the KV cache. 
 - dynamic warmup on typing - in case of long questions, it is a good idea
   to prefill question itself.
 - mixing different models in the same chat sessions. It is possible to send 
   original message to one model and use a different model for the follow-up
   questions.
 - ability to fork the existing chat, start a new branch of the chat and 
   reuse server-side cache for the context we keep.


==============================================================================
2. Installation                                                  *vimqq-install*

vimqq uses |packages| for installation.

Copy over the plugin itself:
>
    git clone https://github.com/okuvshynov/vimqq.git ~/.vim/pack/plugins/start/vimqq

The command above makes vimqq automatically loaded at vim start

Update helptags in vim:
>
    :helptags ~/.vim/pack/plugins/start/vimqq/doc

vimqq will not work in 'compatible' mode. 'nocompatible' needs to be set.

To use local models, get and build llama.cpp server
>
    git clone https://github.com/ggerganov/llama.cpp
    cd llama.cpp
    make -j 16

Download/prepare the models and start llama server:
>
    ./llama.cpp/llama-server
      --model path/to/model.gguf
      --chat-template llama3
      --host 0.0.0.0
      --port 8080

Add a bot endpoint configuration to vimrc file, for example
>
    let g:vqq_llama_servers = [
          \  {'bot_name': 'llama', 'addr': 'http://localhost:8080'},
    \]

It is possible to have multiple bots with different names.

To use claude models, register and get API key. By default vimqq will look for 
API key in environment variable `$ANTHROPIC_API_KEY`. It is possible to 
override the default with 
>
    let g:vqq_claude_api_key = ...

Bot definition looks similar to local llama.cpp:
>
    let g:vqq_claude_models = [
          \  {'bot_name': 'sonnet', 'model': 'claude-3-5-sonnet-20240620'}
    \]

==============================================================================
3. Usage                                                           *vimqq-usage*  

Assuming we have a bot named "llama", we can start by asking a question:
>
    :Q @llama What are basics of unix philosophy?

We can omit the tag as well, and the first configured bot will be used:
>
    :Q What are basics of unix philosophy?

After running this command new window should open in vertical split showing
the message sent and response. If it is local llama bot, we should see
the reply being appended token by token.

Pressing "q" in the window would change the window view to the list of 
past messages. Up/Down or j/k can be used to navigate the list and <cr>
can be used to select individual chat.

The chat titles are generated by the same model as was used to produce
first reply in the chat thread, after the first response has arrived.

Let's add a mapping to demonstrate more complicated use-case.
>
  xnoremap <leader>wst :<C-u>'<,'>QQ -wst @llama<cr>:'<,'>QQ -st @llama 

Now, in visual mode user can select some lines of code, press <leader>wst and 
the following should happen:
  - vimqq will get the visual [s]election;
  - vimqq will look at all c[t]ags available within the selection and collect
    context from destinations;
  - it will send a [w]armup query with selected code, extra context pulled by
    ctags exploration to the server. Server will start processing this
    request and fill the KV cache;
  - it will also prefill the command line with ":'<,'>QQ -st @llama ". User
    can start typing the specific question ('Why does it call foo() instead 
    of bar()').
  - sending a message will hit the same bot which should be able to
    fill in the cache and process our request much faster.

==============================================================================
4. Commands                                                     *vimqq-commands*  

------------------------------------------------------------------------------
4.1. Message sending                                        *vimqq-commands-msg*

First group of commands are used to send messages.
    - `:QQ` sends/warms up a message with range provided and optional arguments 
    - `:Q`  sends/warms up a message with no range and optional arguments 

The format for calling both commands is similar, except for the range part
>
    :Q [-nwsfpt] [@bot_name] message

Supported options:
  - `n` - Send message/warmup in [n]ew chat. If not provided, current chat will
          be continued. If no chat is current, new chat will be still created.
  - `w` - do [w]armup query. Warmup is a oneway query which sole purpose is
          to prefill cache on server. `w` is no-op for claude, which has
          stateless API.
  - `s` - use visual [s]election as context. Unused for 'Q' command.
  - `f` - use current [f]ile as context.
  - `t` - follow c[t]ags from the selection and add some of the found sources
          as context. Unused for 'Q' command.
  - `p` - use entire [p]roject as context. The logic of identifying current
          project is very naive, going up until we find '.git/' directory
          and get all source files (with hardcoded extensions). Can result
          in huge input context.

------------------------------------------------------------------------------
4.1. Forking                                               *vimqq-commands-fork*

    - `:QF` is a slightly different command.

It forks the current chat reusing the context from the first message.
It is useful in cases of long context, but when you want to start a new
discussion thread. For example,
>
    :QF Suggest a simple task for junior engineer working on the project

It will:
  - take current chat's first message, keep the context and bot 
  - modify the question with 'Suggest a ...'
  - create new chat
  - append amended message to new chat
  - send new chat to the original bot
This way we can reuse the context, which might be long (entire file/project)

Note: this command might get merged to the Q/QQ commands. Similar to `n` 
meaning 'ask in new chat' we might add an option to 'ask in fork'.

------------------------------------------------------------------------------
4.1. UI Control                                              *vimqq-commands-ui*

    - `:QQList` will show the list of past chat threads. User can navigate the
      list and select chat session by pressing <CR>. This action will make chat
      session current.

    - `:QQOpenChat chat_id` opens chat with id=`chat_id`. This action will make
      chat session current.

    - `:QQToggle` shows/hides vimqq window.


==============================================================================
5. Mappings                                                     *vimqq-mappings*  

vimqq adds no global key mappings, only the navigation within chat buffer.

In chat list view:
 - 'q' will close the vim-qq window
 - 'd' will show and dialog confirmation and delete chat if user answers Yes.
 - <cr> will select chat session under cursor, open it and make current.

In chat view:
 - 'q' will open the chat list view while keeping the same current chat.

It is useful, however, to define some of your own.

Let's assume we have configured two bots: `llama` and `sonnet`:
>
    let g:vqq_llama_servers = [
          \  {'bot_name': 'llama', 'addr': 'http://localhost:8080'}
    \]
    let g:vqq_claude_models = [
          \  {'bot_name': 'sonnet', 'model': 'claude-3-5-sonnet-20240620'}
    \]

Now we can define some key mappings
>
    " [w]armup llama
    xnoremap <leader>w  :<C-u>'<,'>QQ -ws @llama<cr>:'<,'>QQ -s @llama<Space>
    " [w]armup llama in [n]ew chat
    xnoremap <leader>wn :<C-u>'<,'>QQ -wns @llama<cr>:'<,'>QQ -ns @llama<Space>

    " [q]uery llama
    nnoremap <leader>q :<C-u>Q @llama<Space>
    " [q]uery llama in [n]ew chat
    nnoremap <leader>qn :<C-u>Q -n @llama<Space>

    " [w]armup llama with entire [f]ile context
    nnoremap <leader>wf :<C-u>Q -wf @llama<cr>:Q -f @llama<Space>
    " [w]armup llama with entire [f]ile context in [n]ew chat
    nnoremap <leader>wfn :<C-u>Q -wfn @llama<cr>:Q -fn @llama<Space>

    " [w]armup llama with entire [p]roject context
    nnoremap <leader>wp :<C-u>Q -wp @llama<cr>:Q -p @llama<Space>
    " [w]armup llama with entire [p]roject context in [n]ew chat
    nnoremap <leader>wpn :<C-u>Q -wpn @llama<cr>:Q -pn @llama<Space>

    " [f]ork current chat
    nnoremap <leader>f :<C-u>QF<Space>

    " chat [l]ist
    nnoremap <leader>ll :<C-u>QQList<cr>


==============================================================================
6. Configuration                                                  *vimqq-config*  

Configuration is done using global variables with prefix `g:vqq`.

    - `g:vqq_llama_servers` - list of llama.cpp bots. Default is empty.
      Each bot configuration is a dictionary with the following attributes:
      - `bot_name`. string identification for this bot.
        Must be unique and consist of [a-zA-Z0-9_] symbols. Required
      - `addr`. Path to endpoint, in the http://host:port format. Required.
      - `healthcheck_ms`. How often to do healthcheck query.
        Optional, default is 10000ms (=10s)
      - `title_tokens`. When generating title for chat, up to this many
        tokens can be produced. Optional, default is 16.
      - `max_tokens`. When producing response, up to this many tokens can
        be produced. Optional, default is 1024.
      - `system_prompt`. System prompt to include in the query and steer 
        bot behavior. Optional, default is "You are a helpful assistant".

    - `g:vqq_claude_models` - list of claude bots. Default is empty.
      Each bot configuration is a dictionary with the following attributes:
      - `model`. Model to use, must be one the models supported by Claude.
        Check https://docs.anthropic.com/en/docs/about-claude/models for 
        the model list. Required.
      - `bot_name`. string identification for this bot.
        Must be unique and consist of [a-zA-Z0-9_] symbols. Required
      - `title_tokens`. When generating title for chat, up to this many
        tokens can be produced. Optional, default is 16.
      - `max_tokens`. When producing response, up to this many tokens can
        be produced. Optional, default is 1024.

    - `g:vqq_default_bot`. bot_name of the bot used by default, if user
      omits the tag. Optional, default is first bot.

    - `g:vqq_warmup_on_chat_open`. List of bot names for which to issue 
      a warmup query automatically, every time we opened chat history
      for some chat session. Useful for resuming old sessions. Default
      is empty list.

    - `g:vqq_context_template`. Template to use to construct the message
      if some code needs to be included as a context. Replaces placeholders
      {vvq_ctx} and {vvq_msg} with context (selected code) and message.
      Optional, default is "Here's a code snippet: \n\n{vqq_ctx}\n\n{vqq_msg}"

    - `g:vqq_width`. Default chat window width in characters.
      Optional, defutlt is 80.

    - `g:vqq_time_format`. Time format to use in chat sessions list.
      Optional, default is "%b %d %H:%M ", for example "July 23, 18:05"

    - `g:vqq_chats_file`. Path to json file to store message history.
      Optional, default is expand('~/.vim/vqq_chats.json')

    - `g:vqq_context_filetypes`. When extracring project-level context,
      this list of patterns will be used to include files. 
      Default: "*.vim,*.txt,*.md,*.cpp,*.c,*.h,*.hpp,*.py,*.rs"

    - `g:vqq_autowarm_cmd_ms`. When doing warmup queries vimqq can keep
      sending warmup queries as user types the message. It is useful in
      the cases of longer messages with detailed instructions. This logic
      is based on two events: previous warmup finished and message in the
      command-line has changed. If both of them are true, new warmup will
      be sent. To the best of my knowlegde, vim doesn't provide a way to
      listen to the cmdline update events, but we can check its value
      periodically. This parameter defines this period. Default - 1000ms.

    - `g:vqq_log_file`. File location where to append logs.
      Default is `~/.vim/vimqq.log` 

    - `g:vqq_log_level`. Controls how much to log. Can be one of
      'DEBUG', 'INFO', 'ERROR', 'NONE'. All messages logged with 
      configured level or higher will be appended to the file.


==============================================================================
7. Changelog                                                   *vimqq-changelog*  

 vim:tw=78:ts=8:ft=help:noet:nospell
